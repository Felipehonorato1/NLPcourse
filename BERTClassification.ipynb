{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0zCgAaprBRxgDFb1Iy4Qw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Felipehonorato1/NLPcourse/blob/main/BERTClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc1CNNdFhAbF"
      },
      "source": [
        "!pip install -q tensorflow-text\n",
        "!pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7n7BBe0htBv"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization # Usaremos para criar o otimizador AdamW\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcfpel6niDAo",
        "outputId": "ea51bf19-3022-43b5-ed0b-183b587eee0e"
      },
      "source": [
        " url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        " dataset = tf.keras.utils.get_file('aclImdb_v1.ta.gz', url, untar = True, cache_dir = ' .', cache_subdir ='')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 9s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zDAYQSOinCz",
        "outputId": "ca519b4a-8642-4144-b227-3a07e7d97aa5"
      },
      "source": [
        " dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        " print(dataset_dir)\n",
        "\n",
        " train_dir = os.path.join(dataset_dir, 'train')\n",
        " print(train_dir)\n",
        "\n",
        " test_dir = os.path.join(dataset_dir, 'test')\n",
        " print(test_dir)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/.keras/aclImdb\n",
            "/tmp/.keras/aclImdb/train\n",
            "/tmp/.keras/aclImdb/test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LBsLA65jF41"
      },
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup') # Removendo essa parte dos dados que não usaremos\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifceJGx1kNP3"
      },
      "source": [
        "# Métodos de otimização do algoritmo\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8VwwVaojcn0",
        "outputId": "191f52d2-c25e-4395-aeeb-f7a28d63fafa"
      },
      "source": [
        "# Gerando nossos datasets de treino, validação e teste\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(train_dir, seed = 150, validation_split = 0.2, subset = 'training', batch_size=batch_size)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMF07HCBk3TX",
        "outputId": "27c04202-7087-4c78-bdda-07c0b3271169"
      },
      "source": [
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(train_dir,seed = 150, validation_split = 0.2 ,subset = 'validation', batch_size = batch_size )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaPbLoehlWvo",
        "outputId": "0d229e42-b7b7-4a16-b0f8-2819c5ccee3f"
      },
      "source": [
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(test_dir, batch_size=batch_size)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoEW0-VCkFyv",
        "outputId": "bc24b9ea-42af-4e73-dd75-fbabc7b95e81"
      },
      "source": [
        "class_names = raw_train_ds.class_names\n",
        "print(class_names)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['neg', 'pos']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upNUy_e1kpGl"
      },
      "source": [
        "train_ds = raw_train_ds.cache().prefetch(buffer_size = AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size = AUTOTUNE)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mptGvN-lQ79",
        "outputId": "1154cdab-e852-42bd-85a1-0a6759684163"
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(f'Review: {text_batch.numpy()[i]}')\n",
        "    print(f'Label: {class_names[label_batch.numpy()[i]]}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: b\"I've joined IMDb so people know what a great film this is! It's not often you come across a film that's moving and visually cinematic yet humble. You've read the plot so all I want to say is don't watch it because you want to see a clash of cultural religious identity babble ,because that's the typical misconception people read in to,instead just appreciate and realise it's about a father and son on a voyage growing to know each other through their struggles. Buy it and pass it on before film4 get round to it. This was one of the very few films to be nominated for a BAFTA being independent and foreign. The beauty of it is that it manages to appeal to anyone even if you never watch anything subtitled or just used to the Hollywood formula, just a great story that will keep you engaged. The only thing I wish is for it to be longer and see what happens\"\n",
            "Label: pos\n",
            "Review: b'Before watching this film, I could already tell it was a complete copy of Saw (complete with the shack-like place they were in and the black guy wanting someone to break his hand to get out of the cuffs). MJH\\'s name on a movie would typically turn me away (ugh, can we say GROSS?!), but I still wanted to give it a try.<br /><br />Starting out, I was a bit interested. The acting is absolutely horrible and I found myself laughing at almost each reaction from the characters (especially the man that played \"Sulley\"). MJH was even worst, but I continued to watch.<br /><br />However, the ending was the biggest joke of them all! I seriously sat in shock thinking \"THAT was the ending?! Is this a comedy?!\".<br /><br />I thought this pile of crap was funnier than the \"Scary Movie\" spoofs and that is REALLY saying something!'\n",
            "Label: neg\n",
            "Review: b'This movie was so badly written, directed and acted that it beggars belief. It should be remade with a better script, director and casting service. The worst problem is the acting. You have Jennifer Beals on the one hand who is polished, professional and totally believable, and on the other hand, Ri\\'chard, who is woefully miscast and just jarring in this particular piece. Peter Gallagher and Jenny Levine are just awful as the slave owning (and keeping) couple, although both normally do fine work. The actors (and director) should not have attempted to do accents at all--they are inconsistent and unbelievable. Much better to have concentrated on doing a good job in actual English. The casting is ludicrous. Why have children of an \"African\" merchant (thus less socially desirable to the gens de couleur society ) been cast with very pale skinned actors, while the supposedly socially desirable Marcel, has pronounced African features, including an obviously dyed blond \"fro\"? It\\'s as if the casting directors cannot be bothered to read the script they are casting and to chose appropriate actors from a large pool of extremely talented and physically diverse actors of color. It\\'s just so weird! This could be a great movie and should be re-made, but with people who respect the material and can choose appropriate and skilled actors. There are plenty of good actors out there, and it would be fun to see how Jennifer Beals, Daniel Sunjata and Gloria Reuben would do with an appropriate cast, good script and decent direction.'\n",
            "Label: neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKm2478ImtWK"
      },
      "source": [
        "# Usando o modelo BERT EN UNCASED \n",
        "def bert_embedding_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string) # Já que não temos tamanho definido para as strings passamos shape vazio\n",
        "  preprocessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
        "\n",
        "  encoder_inputs = preprocessor(text_input)\n",
        "\n",
        "  encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\", trainable=True)\n",
        "\n",
        "  outputs = encoder(encoder_inputs)\n",
        "\n",
        "  pooled_output = outputs['pooled_output']\n",
        "  sequence_output = outputs = outputs['sequence_output']\n",
        "  model = tf.keras.Model(text_input, pooled_outputs)\n",
        "\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEuQJwYfpQY3"
      },
      "source": [
        "embedding_model = bert_embedding_model()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0KXPAw5pU06"
      },
      "source": [
        "sentences = tf.constant(['This is a test'])\n",
        "processed_sentences = embedding_model(sentences)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bahjQCIqpsFP"
      },
      "source": [
        "# BERT UNCASED MODEL\n",
        "def classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name = 'text input') # Já que não temos tamanho definido para as strings passamos shape vazio\n",
        "  preprocessed = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',name = 'Preprocessing_layer')(text_input)\n",
        "\n",
        "  encoder_outputs = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\", trainable=True, name = 'Encoding')(preprocessed)\n",
        "\n",
        "  pooled_output = encoder_outputs['pooled_output']\n",
        "  \n",
        "  net = tf.keras.layers.Dropout(0.1, name = 'Dropout_layer')(pooled_output)\n",
        "  net = tf.keras.layers.Dense(16, activation ='relu', name = 'Dense_net')(net)\n",
        "  net = tf.keras.layers.Dense(1,activation = 'sigmoid', name = 'Classifier')(net)\n",
        "\n",
        "  model = tf.keras.Model(text_input, net)\n",
        "\n",
        " \n",
        "\n",
        "  init_lr = 3e-5\n",
        "  epochs = 5\n",
        "  steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "  num_train_steps = steps_per_epoch * epochs\n",
        "  num_warmup_steps = int(0.1*num_train_steps)\n",
        "  optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "  metrics = tf.metrics.BinaryAccuracy()\n",
        "  model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
        "  return model\n",
        "\n",
        "  "
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX_S_h37sq5h"
      },
      "source": [
        "classifier = classifier_model()"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKQiwUmEswIe",
        "outputId": "286cc102-3dd9-408a-85a4-560cfa870f06"
      },
      "source": [
        "history = classifier.fit(x = train_ds, validation_data= val_ds, epochs = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "625/625 [==============================] - 1178s 2s/step - loss: 0.5118 - binary_accuracy: 0.7138 - val_loss: 0.2873 - val_binary_accuracy: 0.8782\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2678 - binary_accuracy: 0.8915"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WazzjJrMODpD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}